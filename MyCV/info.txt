#!/bin/bash

# Set environment variables
export STORM_AUTH_SIMPLE_USERNAME="admin" export
STORM_AUTH_SIMPLE_PASSWORD="password"

# Set Java options to pass environment variables as system properties
export
STORM_JAVA_OPTS="-Dstorm.auth.simple.username=$STORM_AUTH_SIMPLE_USERNAME -Dstorm.auth.simple.password=$STORM_AUTH_SIMPLE_PASSWORD"

# Start Storm UI (modify paths as necessary)
storm nimbus & storm ui & storm supervisor &

ui.filter: "org.apache.hadoop.security.authentication.server.AuthenticationFilter"
ui.filter.params:
  "config.prefix": "storm.auth"
  "storm.auth.type": "simple"
  "storm.auth.simple.anonymous.allowed": "false"
  "storm.auth.simple.username": "${storm.auth.simple.username}"
  "storm.auth.simple.password": "${storm.auth.simple.password}"

Setting up NGINX as a reverse proxy to forward requests to the Storm UI and
require a user.name parameter can be done by configuring NGINX to handle
authentication and then pass the requests to the Storm UI.

sudo nano /etc/nginx/conf.d/storm.conf

htpasswd -c /etc/nginx/.htpasswd admin

server { listen 80; server_name your-server-name;

    location / {
        # Restrict access to localhost only allow 127.0.0.1; deny all;
        # Enable Basic Authentication auth_basic "Restricted Access";
          auth_basic_user_file /etc/nginx/.htpasswd;
        # Check if user.name is present in the query string if
          ($arg_user_name = "") { return 403; }

        # Pass the user.name parameter to the backend server proxy_set_header
          user.name $arg_user_name;

        # Proxy settings to forward requests to Storm UI proxy_pass
          http://localhost:8080;  # Change the port if your Storm UI runs on a
          different port proxy_set_header Host $host; proxy_set_header
          X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For
          $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto
          $scheme;

        # Additional proxy settings proxy_http_version 1.1; proxy_set_header
          Connection ""; chunked_transfer_encoding off; } }

Test the NGINX configuration for syntax errors: sudo nginx -t

sudo systemctl reload nginx

http://your-server-name/?user_name=admin

By following these steps, you have configured NGINX to act as a reverse proxy
for the Storm UI and enforce the presence of the user.name query parameter.
This setup ensures that any request without the user.name parameter is
rejected, enhancing security by requiring this parameter for access.


# Ensure your firewall is configured to block access to port 8888 from external
# machines.
sudo iptables -A INPUT -p tcp --dport 8888 -j DROP sudo iptables -A INPUT -p
tcp -s 127.0.0.1 --dport 8888 -j ACCEPT
-----------------------------------------------
ui.port: 8744 # Default UI port, change as necessary

ui.https.enabled: true ui.https.keystore.path: "path/to/keystore.jks"
ui.https.keystore.password: "keystorepassword" ui.https.keystore.type: "JKS"
ui.https.key.password: "keypassword"

# Enable client authentication
ui.https.need.client.auth: true
ui.https.truststore.path: "path/to/truststore.jks"
ui.https.truststore.password: "truststorepassword"
ui.https.truststore.type: "JKS"


# Creating an Index with Mappings
curl -X PUT "localhost:9200/my_index" -H 'Content-Type: application/json' -d'{
  "mappings": {
    "properties": {
      "name": {
        "type": "text" },
      "age": {
        "type": "integer" },
      "email": {
        "type": "keyword" } } } }'

# Verifying Mappings
curl -X GET "localhost:9200/my_index/_mapping" -H 'Content-Type:
application/json'


# polkit
Letâ€™s create a Polkit rule that grants permissions to manage example.service,
another.service, and yetanother.service for a user named username. We will
write this rule in JavaScript, which is the language used for Polkit rules
since version 0.106.

sudo nano /etc/polkit-1/rules.d/50-systemctl-multi-manage.rules

polkit.addRule(function(action, subject) {
    var services = ["example.service", "another.service", "yetanother.service"];  // List of services the user can manage
    if (action.id.indexOf("org.freedesktop.systemd1.manage-unit-files") >= 0 ||
        action.id.indexOf("org.freedesktop.systemd1.manage-units") >= 0) {
        if (services.indexOf(action.lookup("unit")) >= 0 && subject.user == "username") {
            return polkit.Result.YES;
        }
    }
});

# several users
polkit.addRule(function(action, subject) {
    var allowedServices = ["example.service", "another.service", "yetanother.service"];  // List of services
    var allowedUsers = ["username1", "username2", "username3"];  // List of users

    if ((action.id.indexOf("org.freedesktop.systemd1.manage-unit-files") >= 0 ||
        action.id.indexOf("org.freedesktop.systemd1.manage-units") >= 0) &&
        allowedServices.indexOf(action.lookup("unit")) >= 0 &&
        allowedUsers.indexOf(subject.user) >= 0) {
        return polkit.Result.YES;
    }
});


polkit.addRule(function(action, subject) {
    if (action.id == "org.freedesktop.systemd1.manage-units" &&
        subject.isInGroup("wheel")) {
        return polkit.Result.YES;
    }
});


# test
sudo systemctl restart example.service

/etc/sudoers
username ALL=NOPASSWD: /bin/systemctl start example.service, /bin/systemctl stop example.service, /bin/systemctl restart example.service

username ALL=(ALL) NOPASSWD: /bin/systemctl start *, /bin/systemctl stop *, /bin/systemctl restart *, /bin/systemctl status *

---------------------
const dbName = "yourDb";
const collName = "yourCollection";
const sourceShard = "shard01";
const targetShard = "shard02";
const dryRun = true; // ðŸ”„ Set to false to actually move chunks

const configDB = db.getSiblingDB("config");

// Step 1: Get the collection UUID
const collEntry = configDB.collections.findOne({ _id: `${dbName}.${collName}` });
if (!collEntry) {
  throw new Error(`Collection ${dbName}.${collName} not found in config.collections`);
}

const uuid = collEntry.uuid;
print(`âœ… Collection UUID: ${uuid}`);

// Step 2: Get chunks for the UUID and source shard
const chunks = configDB.chunks.find({ uuid: uuid, shard: sourceShard }).toArray();
print(`ðŸ“¦ Found ${chunks.length} chunks on ${sourceShard}`);

// Step 3: Iterate and move chunks (or just show what would be moved)
chunks.forEach(chunk => {
  const key = chunk.min;
  if (dryRun) {
    print(`ðŸ§ª [Dry Run] Would move chunk: min=${tojson(key)} from ${sourceShard} to ${targetShard}`);
  } else {
    print(`ðŸšš Moving chunk: min=${tojson(key)} to ${targetShard}`);
    const result = sh.moveChunk(`${dbName}.${collName}`, key, targetShard);
    if (result.ok) {
      print(`âœ… Moved chunk: ${tojson(key)}`);
    } else {
      print(`âŒ Failed to move chunk: ${tojson(result)}`);
    }
  }
});

sh.addShardToZone("shard02", "zoneA")
sh.addShardToZone("shard03", "zoneA")

sh.updateZoneKeyRange(
  "mydb.mycoll",
  { _id: MinKey },
  { _id: MaxKey },
  "zoneA"
)

// move chunks or let balancer do it:
sh.startBalancer()



// check chunk distribution
db.getSiblingDB("config").chunks.aggregate([
  { $group: { _id: "$shard", count: { $sum: 1 } } }
]);

// check assigned zones
sh.status()

const replSetName = "rsNewShard";
const shardMembers = [
  { host: "host1:27017" },
  { host: "host2:27017" },
  { host: "host3:27017" }
];

const mongosUser = "admin";
const mongosPwd = "password";
const primaryHost = shardMembers[0].host;

// Step 1: Connect to mongos
const mongosAdmin = new Mongo().getDB("admin");
mongosAdmin.auth(mongosUser, mongosPwd);

let addShardResult;
try {
  addShardResult = mongosAdmin.runCommand({
    addShard: `${replSetName}/${shardMembers.map(m => m.host).join(",")}`
  });

  if (addShardResult.ok) {
    print("Shard added successfully.");
    quit();
  } else if (addShardResult.codeName === "DuplicateKey" || /already exists/.test(addShardResult.errmsg)) {
    print("Shard already exists, continuing to add missing members...");
  } else {
    throw new Error("Unhandled error during addShard: " + tojson(addShardResult));
  }
} catch (e) {
  print("Error during addShard:", e.message);
}

// Step 2: Connect to replica set primary and add missing members
const rsConn = new Mongo(primaryHost);
const rsAdmin = rsConn.getDB("admin");
rsAdmin.auth(mongosUser, mongosPwd);

const status = rsAdmin.runCommand({ replSetGetStatus: 1 });
const existingHosts = status.members.map(m => m.name);

// Add each missing member
shardMembers.forEach(m => {
  if (!existingHosts.includes(m.host)) {
    print(`Adding member ${m.host} to replica set ${replSetName}`);
    const addResult = rsAdmin.runCommand({ replSetAdd: { host: m.host } });

    if (!addResult.ok) {
      print(`Failed to add ${m.host}: ${tojson(addResult)}`);
    } else {
      print(`Successfully added ${m.host}`);
    }
  } else {
    print(`${m.host} already exists in replica set`);
  }
});


--------------------------



package main

import (
    "context"
    "fmt"
    "log"
    "sync"
    "time"

    "go.mongodb.org/mongo-driver/bson"
    "go.mongodb.org/mongo-driver/mongo"
    "go.mongodb.org/mongo-driver/mongo/options"
)

// DeleteInBatchesConcurrent deletes matching documents in concurrent batches.
func DeleteInBatchesConcurrent(ctx context.Context, client *mongo.Client, dbName, collName string, filter bson.M, batchSize int, concurrency int) error {
    session, err := client.StartSession()
    if err != nil {
        return fmt.Errorf("start session failed: %w", err)
    }
    defer session.EndSession(ctx)

    return mongo.WithSession(ctx, session, func(sc mongo.SessionContext) error {
        ticker := time.NewTicker(5 * time.Minute)
        defer ticker.Stop()

        // Keep session alive
        go func() {
            for range ticker.C {
                _ = client.Database("admin").RunCommand(sc, bson.D{{Key: "ping", Value: 1}}).Err()
            }
        }()

        collection := client.Database(dbName).Collection(collName)
        idChan := make(chan interface{}, batchSize*concurrency)
        wg := sync.WaitGroup{}

        // Worker goroutines
        for i := 0; i < concurrency; i++ {
            wg.Add(1)
            go func() {
                defer wg.Done()
                var ids []interface{}
                for id := range idChan {
                    ids = append(ids, id)
                    if len(ids) == batchSize {
                        deleteBatch(sc, collection, ids)
                        ids = nil
                    }
                }
                if len(ids) > 0 {
                    deleteBatch(sc, collection, ids)
                }
            }()
        }

        // Producer: Fetch IDs and push into idChan
        for {
            cursor, err := collection.Find(sc, filter, options.Find().SetLimit(int64(batchSize*concurrency)).SetProjection(bson.M{"_id": 1}))
            if err != nil {
                return fmt.Errorf("find failed: %w", err)
            }

            count := 0
            for cursor.Next(sc) {
                var doc bson.M
                if err := cursor.Decode(&doc); err != nil {
                    cursor.Close(sc)
                    return fmt.Errorf("decode failed: %w", err)
                }
                idChan <- doc["_id"]
                count++
            }
            cursor.Close(sc)

            if count == 0 {
                break
            }
        }

        close(idChan)
        wg.Wait()

        return nil
    })
}

// Helper to delete a batch of documents by _id
func deleteBatch(ctx context.Context, coll *mongo.Collection, ids []interface{}) {
    _, err := coll.DeleteMany(ctx, bson.M{"_id": bson.M{"$in": ids}})
    if err != nil {
        log.Printf("Error deleting batch: %v", err)
    } else {
        log.Printf("Deleted %d documents", len(ids))
    }
}



func main() {
    ctx := context.Background()
    client, err := mongo.Connect(ctx, options.Client().ApplyURI("mongodb://localhost:27017"))
    if err != nil {
        log.Fatal(err)
    }

    filter := bson.M{"status": "obsolete"}
    err = DeleteInBatchesConcurrent(ctx, client, "testdb", "mycollection", filter, 500, 4)
    if err != nil {
        log.Fatal(err)
    }
}

